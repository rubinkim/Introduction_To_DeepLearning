{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random, seed\n",
    "from math import exp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Initialize Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    # Create a list named 'network' in which I contain hidden and output layer\n",
    "    network = []\n",
    "    \n",
    "    # Create a hidden layer\n",
    "    hidden_layer = [{'weights' : [random() for i in range(n_inputs+1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    \n",
    "    # Create a output layer\n",
    "    output_layer = [{'weights' : [random() for i in range(n_hidden+1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Forward Propagate Inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2-1. Neuron Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate neuron activaton for an input\n",
    "def activate(weights, inputs):\n",
    "    # Include the bias for calculation\n",
    "    activation = weights[-1]\n",
    "    for i in range(len(weights)-1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2-2. Neuron Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer neuron activation\n",
    "def transfer(activation):\n",
    "    # Sigmoid(Logistic) function is applied\n",
    "    return 1.0 / (1.0 + exp(-activation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2-3. Forward Propagate input to a network output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(network, row):\n",
    "    # Create a variable 'input' to forward propagate the neurons in the input layer to the hidden layer\n",
    "    # and save the values in row to the variable 'input'.\n",
    "    input = row\n",
    "    \n",
    "    # Loop the network list to access the layers sequentially\n",
    "    for layer in network:\n",
    "        # Create a variable 'new_inputs' to contain the values that result from activate and transfer.\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], input)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "            \n",
    "        # We need this to be used as the input to the next layer\n",
    "        input = new_inputs\n",
    "        \n",
    "    return network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Back Propagate Error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3-1. Transfer Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_derivate(output):\n",
    "    # Sigmoid(Logistic) funtion is used.\n",
    "    return output * (1.0 - output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3-2. Error Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagate_error(network, expected):\n",
    "    for i in range(len(network)-1, -1, -1):\n",
    "        \n",
    "        # Start with the output layer because outputs flows backwards from output layer to input layer.\n",
    "        layer = network[i]   # output layer\n",
    "        # error_signal = error * transfer_derivative(output) for the output layer\n",
    "        errors, error_signals = [],[]  \n",
    "        \n",
    "        if i == len(network) - 1:\n",
    "            for k, neuron in enumerate(layer):\n",
    "                error = neuron['output'] - expected[k]\n",
    "                error_signal = error * neuron['delta']\n",
    "                errors.append(error)\n",
    "                error_signals.append(error_signal)\n",
    "                neuron['delta'] = error_signal\n",
    "                \n",
    "        else:\n",
    "            for k, neuron in enumerate(layer):\n",
    "                error = 0\n",
    "                for j, next_neuron in enumerate(network[i+1]):\n",
    "                    error += next_neuron['weights'][k] * next_neuron['delta']\n",
    "                    error_signal = error * transfer_derivate(neuron['output'])\n",
    "                    errors.append(error)\n",
    "                    error_signals.append(error_signal)\n",
    "                    neuron['delta'] = error_signal               "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b18c970f1c267b2ae7de1f51c7b9ea29dc67f6c9f87d5b0ef47c4534e0830b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
