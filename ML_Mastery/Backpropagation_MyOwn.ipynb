{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import random, seed\n",
    "from math import exp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Initialize Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    # Create a list named 'network' in which I contain hidden and output layer\n",
    "    network = []\n",
    "    \n",
    "    # Create a hidden layer\n",
    "    hidden_layer = [{'weights' : [random() for i in range(n_inputs+1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    \n",
    "    # Create a output layer\n",
    "    output_layer = [{'weights' : [random() for i in range(n_hidden+1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Forward Propagate Inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2-1. Neuron Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate neuron activaton for an input\n",
    "def activate(weights, inputs):\n",
    "    # Include the bias for calculation\n",
    "    activation = weights[-1]\n",
    "    for i in range(len(weights)-1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2-2. Neuron Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer neuron activation\n",
    "def transfer(activation):\n",
    "    # Sigmoid(Logistic) function is applied\n",
    "    return 1.0 / (1.0 + exp(-activation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2-3. Forward Propagate input to a network output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(network, row):\n",
    "    # Create a variable 'input' to forward propagate the neurons in the input layer to the hidden layer\n",
    "    # and save the values in row to the variable 'input'.\n",
    "    inputs = row\n",
    "    \n",
    "    # Loop the network list to access the layers sequentially\n",
    "    for layer in network:\n",
    "        # Create a variable 'new_inputs' to contain the values that result from activate and transfer.\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "            \n",
    "        # We need this to be used as the input to the next layer\n",
    "        inputs = new_inputs\n",
    "        \n",
    "    return inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Back Propagate Error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3-1. Transfer Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_derivative(output):\n",
    "    # Sigmoid(Logistic) funtion is used.\n",
    "    return output * (1.0 - output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3-2. Error Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagate_error(network, expected):\n",
    "    for i in range(len(network)-1, -1, -1):\n",
    "        \n",
    "        # Start with the output layer because outputs flows backwards from output layer to input layer.\n",
    "        layer = network[i]   # output layer\n",
    "        # error_signal = error * transfer_derivative(output) for the output layer\n",
    "        errors, error_signals = [],[]  \n",
    "        \n",
    "        for k, neuron in enumerate(layer):\n",
    "            if i == len(network) - 1:\n",
    "                error = neuron['output'] - expected[k]\n",
    "                error_signal = error * transfer_derivative(neuron['output'])\n",
    "            else:\n",
    "                error = 0.0\n",
    "                for j, next_neuron in enumerate(network[i+1]):\n",
    "                    error += next_neuron['weights'][k] * next_neuron['delta']\n",
    "                    error_signal = error * transfer_derivative(neuron['output'])\n",
    "                    \n",
    "            errors.append(error)\n",
    "            error_signals.append(error_signal)\n",
    "            neuron['delta'] = error_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614], 'delta': 0.0005348048046610517}]\n",
      "[{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095], 'delta': 0.14619064683582808}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381, 0.651592972722763], 'delta': -0.0771723774346327}]\n"
     ]
    }
   ],
   "source": [
    "# test backpropagation of error\n",
    "network = [[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n",
    " [{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095]}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381, 0.651592972722763]}]]\n",
    "expected = [0, 1]\n",
    "\n",
    "backward_propagate_error(network, expected)\n",
    "for layer in network:\n",
    "    print(layer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Train Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-1. Update network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "    for i, layer in enumerate(network):\n",
    "        if i == 0:\n",
    "            inputs = row[:-1]     # last element is the target variable\n",
    "        else:\n",
    "            inputs = [neuron['output'] for neuron in network[i-1]]\n",
    "            \n",
    "        for neuron in layer:\n",
    "            for k, input in enumerate(inputs):\n",
    "                neuron['weights'][k] -= l_rate * neuron['delta'] * input\n",
    "                neuron['weights'][-1] -- l_rate * neuron['delta']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "    # Implement Stochastic Gradient Descent\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        # For each epoch, train the network by minimizing sum_error\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[row[-1]] = 1 \n",
    "            \n",
    "            sum_error += sum([(expected[i] - outputs[i]) ** 2 for i in range(len(expected))])\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate) \n",
    "            print(\">epoch=%d, lrate=%.3f, error=%.3f\"% (epoch, l_rate, sum_error))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test training backprop algorithm\n",
    "seed(1)\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    " [1.465489372,2.362125076,0],\n",
    " [3.396561688,4.400293529,0],\n",
    " [1.38807019,1.850220317,0],\n",
    " [3.06407232,3.005305973,0],\n",
    " [7.627531214,2.759262235,1],\n",
    " [5.332441248,2.088626775,1],\n",
    " [6.922596716,1.77106367,1],\n",
    " [8.675418651,-0.242068655,1],\n",
    " [7.673756466,3.508563011,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.500, error=0.634\n",
      ">epoch=0, lrate=0.500, error=1.228\n",
      ">epoch=0, lrate=0.500, error=1.797\n",
      ">epoch=0, lrate=0.500, error=2.313\n",
      ">epoch=0, lrate=0.500, error=2.799\n",
      ">epoch=0, lrate=0.500, error=3.621\n",
      ">epoch=0, lrate=0.500, error=4.395\n",
      ">epoch=0, lrate=0.500, error=5.132\n",
      ">epoch=0, lrate=0.500, error=5.800\n",
      ">epoch=0, lrate=0.500, error=6.467\n",
      ">epoch=1, lrate=0.500, error=0.577\n",
      ">epoch=1, lrate=0.500, error=1.108\n",
      ">epoch=1, lrate=0.500, error=1.604\n",
      ">epoch=1, lrate=0.500, error=2.056\n",
      ">epoch=1, lrate=0.500, error=2.468\n",
      ">epoch=1, lrate=0.500, error=3.251\n",
      ">epoch=1, lrate=0.500, error=3.970\n",
      ">epoch=1, lrate=0.500, error=4.634\n",
      ">epoch=1, lrate=0.500, error=5.203\n",
      ">epoch=1, lrate=0.500, error=5.790\n",
      ">epoch=2, lrate=0.500, error=0.553\n",
      ">epoch=2, lrate=0.500, error=1.059\n",
      ">epoch=2, lrate=0.500, error=1.522\n",
      ">epoch=2, lrate=0.500, error=1.949\n",
      ">epoch=2, lrate=0.500, error=2.333\n",
      ">epoch=2, lrate=0.500, error=3.066\n",
      ">epoch=2, lrate=0.500, error=3.728\n",
      ">epoch=2, lrate=0.500, error=4.326\n",
      ">epoch=2, lrate=0.500, error=4.807\n",
      ">epoch=2, lrate=0.500, error=5.353\n",
      ">epoch=3, lrate=0.500, error=0.533\n",
      ">epoch=3, lrate=0.500, error=1.020\n",
      ">epoch=3, lrate=0.500, error=1.459\n",
      ">epoch=3, lrate=0.500, error=1.871\n",
      ">epoch=3, lrate=0.500, error=2.241\n",
      ">epoch=3, lrate=0.500, error=2.925\n",
      ">epoch=3, lrate=0.500, error=3.537\n",
      ">epoch=3, lrate=0.500, error=4.060\n",
      ">epoch=3, lrate=0.500, error=4.498\n",
      ">epoch=3, lrate=0.500, error=5.010\n",
      ">epoch=4, lrate=0.500, error=0.495\n",
      ">epoch=4, lrate=0.500, error=0.938\n",
      ">epoch=4, lrate=0.500, error=1.335\n",
      ">epoch=4, lrate=0.500, error=1.719\n",
      ">epoch=4, lrate=0.500, error=2.071\n",
      ">epoch=4, lrate=0.500, error=2.652\n",
      ">epoch=4, lrate=0.500, error=3.191\n",
      ">epoch=4, lrate=0.500, error=3.652\n",
      ">epoch=4, lrate=0.500, error=4.087\n",
      ">epoch=4, lrate=0.500, error=4.550\n",
      ">epoch=5, lrate=0.500, error=0.440\n",
      ">epoch=5, lrate=0.500, error=0.811\n",
      ">epoch=5, lrate=0.500, error=1.145\n",
      ">epoch=5, lrate=0.500, error=1.475\n",
      ">epoch=5, lrate=0.500, error=1.787\n",
      ">epoch=5, lrate=0.500, error=2.309\n",
      ">epoch=5, lrate=0.500, error=2.836\n",
      ">epoch=5, lrate=0.500, error=3.291\n",
      ">epoch=5, lrate=0.500, error=3.726\n",
      ">epoch=5, lrate=0.500, error=4.186\n",
      ">epoch=6, lrate=0.500, error=0.382\n",
      ">epoch=6, lrate=0.500, error=0.681\n",
      ">epoch=6, lrate=0.500, error=0.950\n",
      ">epoch=6, lrate=0.500, error=1.224\n",
      ">epoch=6, lrate=0.500, error=1.489\n",
      ">epoch=6, lrate=0.500, error=1.994\n",
      ">epoch=6, lrate=0.500, error=2.522\n",
      ">epoch=6, lrate=0.500, error=2.974\n",
      ">epoch=6, lrate=0.500, error=3.407\n",
      ">epoch=6, lrate=0.500, error=3.867\n",
      ">epoch=7, lrate=0.500, error=0.329\n",
      ">epoch=7, lrate=0.500, error=0.569\n",
      ">epoch=7, lrate=0.500, error=0.786\n",
      ">epoch=7, lrate=0.500, error=1.012\n",
      ">epoch=7, lrate=0.500, error=1.237\n",
      ">epoch=7, lrate=0.500, error=1.729\n",
      ">epoch=7, lrate=0.500, error=2.258\n",
      ">epoch=7, lrate=0.500, error=2.706\n",
      ">epoch=7, lrate=0.500, error=3.139\n",
      ">epoch=7, lrate=0.500, error=3.596\n",
      ">epoch=8, lrate=0.500, error=0.286\n",
      ">epoch=8, lrate=0.500, error=0.479\n",
      ">epoch=8, lrate=0.500, error=0.656\n",
      ">epoch=8, lrate=0.500, error=0.843\n",
      ">epoch=8, lrate=0.500, error=1.036\n",
      ">epoch=8, lrate=0.500, error=1.517\n",
      ">epoch=8, lrate=0.500, error=2.046\n",
      ">epoch=8, lrate=0.500, error=2.492\n",
      ">epoch=8, lrate=0.500, error=2.923\n",
      ">epoch=8, lrate=0.500, error=3.379\n",
      ">epoch=9, lrate=0.500, error=0.250\n",
      ">epoch=9, lrate=0.500, error=0.407\n",
      ">epoch=9, lrate=0.500, error=0.553\n",
      ">epoch=9, lrate=0.500, error=0.710\n",
      ">epoch=9, lrate=0.500, error=0.876\n",
      ">epoch=9, lrate=0.500, error=1.349\n",
      ">epoch=9, lrate=0.500, error=1.876\n",
      ">epoch=9, lrate=0.500, error=2.320\n",
      ">epoch=9, lrate=0.500, error=2.751\n",
      ">epoch=9, lrate=0.500, error=3.206\n",
      ">epoch=10, lrate=0.500, error=0.220\n",
      ">epoch=10, lrate=0.500, error=0.350\n",
      ">epoch=10, lrate=0.500, error=0.471\n",
      ">epoch=10, lrate=0.500, error=0.604\n",
      ">epoch=10, lrate=0.500, error=0.749\n",
      ">epoch=10, lrate=0.500, error=1.215\n",
      ">epoch=10, lrate=0.500, error=1.739\n",
      ">epoch=10, lrate=0.500, error=2.182\n",
      ">epoch=10, lrate=0.500, error=2.612\n",
      ">epoch=10, lrate=0.500, error=3.066\n",
      ">epoch=11, lrate=0.500, error=0.195\n",
      ">epoch=11, lrate=0.500, error=0.303\n",
      ">epoch=11, lrate=0.500, error=0.405\n",
      ">epoch=11, lrate=0.500, error=0.519\n",
      ">epoch=11, lrate=0.500, error=0.647\n",
      ">epoch=11, lrate=0.500, error=1.108\n",
      ">epoch=11, lrate=0.500, error=1.628\n",
      ">epoch=11, lrate=0.500, error=2.069\n",
      ">epoch=11, lrate=0.500, error=2.500\n",
      ">epoch=11, lrate=0.500, error=2.952\n",
      ">epoch=12, lrate=0.500, error=0.173\n",
      ">epoch=12, lrate=0.500, error=0.265\n",
      ">epoch=12, lrate=0.500, error=0.352\n",
      ">epoch=12, lrate=0.500, error=0.451\n",
      ">epoch=12, lrate=0.500, error=0.564\n",
      ">epoch=12, lrate=0.500, error=1.021\n",
      ">epoch=12, lrate=0.500, error=1.537\n",
      ">epoch=12, lrate=0.500, error=1.977\n",
      ">epoch=12, lrate=0.500, error=2.407\n",
      ">epoch=12, lrate=0.500, error=2.859\n",
      ">epoch=13, lrate=0.500, error=0.155\n",
      ">epoch=13, lrate=0.500, error=0.234\n",
      ">epoch=13, lrate=0.500, error=0.309\n",
      ">epoch=13, lrate=0.500, error=0.395\n",
      ">epoch=13, lrate=0.500, error=0.497\n",
      ">epoch=13, lrate=0.500, error=0.950\n",
      ">epoch=13, lrate=0.500, error=1.462\n",
      ">epoch=13, lrate=0.500, error=1.901\n",
      ">epoch=13, lrate=0.500, error=2.331\n",
      ">epoch=13, lrate=0.500, error=2.782\n",
      ">epoch=14, lrate=0.500, error=0.140\n",
      ">epoch=14, lrate=0.500, error=0.208\n",
      ">epoch=14, lrate=0.500, error=0.274\n",
      ">epoch=14, lrate=0.500, error=0.350\n",
      ">epoch=14, lrate=0.500, error=0.441\n",
      ">epoch=14, lrate=0.500, error=0.891\n",
      ">epoch=14, lrate=0.500, error=1.400\n",
      ">epoch=14, lrate=0.500, error=1.838\n",
      ">epoch=14, lrate=0.500, error=2.268\n",
      ">epoch=14, lrate=0.500, error=2.717\n",
      ">epoch=15, lrate=0.500, error=0.127\n",
      ">epoch=15, lrate=0.500, error=0.187\n",
      ">epoch=15, lrate=0.500, error=0.244\n",
      ">epoch=15, lrate=0.500, error=0.312\n",
      ">epoch=15, lrate=0.500, error=0.395\n",
      ">epoch=15, lrate=0.500, error=0.843\n",
      ">epoch=15, lrate=0.500, error=1.347\n",
      ">epoch=15, lrate=0.500, error=1.785\n",
      ">epoch=15, lrate=0.500, error=2.214\n",
      ">epoch=15, lrate=0.500, error=2.663\n",
      ">epoch=16, lrate=0.500, error=0.116\n",
      ">epoch=16, lrate=0.500, error=0.169\n",
      ">epoch=16, lrate=0.500, error=0.219\n",
      ">epoch=16, lrate=0.500, error=0.280\n",
      ">epoch=16, lrate=0.500, error=0.356\n",
      ">epoch=16, lrate=0.500, error=0.802\n",
      ">epoch=16, lrate=0.500, error=1.303\n",
      ">epoch=16, lrate=0.500, error=1.740\n",
      ">epoch=16, lrate=0.500, error=2.169\n",
      ">epoch=16, lrate=0.500, error=2.617\n",
      ">epoch=17, lrate=0.500, error=0.107\n",
      ">epoch=17, lrate=0.500, error=0.153\n",
      ">epoch=17, lrate=0.500, error=0.199\n",
      ">epoch=17, lrate=0.500, error=0.253\n",
      ">epoch=17, lrate=0.500, error=0.324\n",
      ">epoch=17, lrate=0.500, error=0.767\n",
      ">epoch=17, lrate=0.500, error=1.265\n",
      ">epoch=17, lrate=0.500, error=1.701\n",
      ">epoch=17, lrate=0.500, error=2.131\n",
      ">epoch=17, lrate=0.500, error=2.577\n",
      ">epoch=18, lrate=0.500, error=0.099\n",
      ">epoch=18, lrate=0.500, error=0.140\n",
      ">epoch=18, lrate=0.500, error=0.181\n",
      ">epoch=18, lrate=0.500, error=0.231\n",
      ">epoch=18, lrate=0.500, error=0.295\n",
      ">epoch=18, lrate=0.500, error=0.738\n",
      ">epoch=18, lrate=0.500, error=1.232\n",
      ">epoch=18, lrate=0.500, error=1.668\n",
      ">epoch=18, lrate=0.500, error=2.097\n",
      ">epoch=18, lrate=0.500, error=2.543\n",
      ">epoch=19, lrate=0.500, error=0.091\n",
      ">epoch=19, lrate=0.500, error=0.129\n",
      ">epoch=19, lrate=0.500, error=0.166\n",
      ">epoch=19, lrate=0.500, error=0.211\n",
      ">epoch=19, lrate=0.500, error=0.271\n",
      ">epoch=19, lrate=0.500, error=0.713\n",
      ">epoch=19, lrate=0.500, error=1.204\n",
      ">epoch=19, lrate=0.500, error=1.639\n",
      ">epoch=19, lrate=0.500, error=2.068\n",
      ">epoch=19, lrate=0.500, error=2.514\n",
      "[{'weights': [-1.4111124009996667, 1.596133118217364, 0.763774618976614], 'output': 0.013844516687858265, 'delta': 0.005581727759811537}, {'weights': [-0.7962538972739899, 0.4888157066918689, 0.4494910647887381], 'output': 0.020814826143113987, 'delta': 0.002659439938405591}]\n",
      "\n",
      "[{'weights': [1.4634124918573872, 0.6506001031726008, 0.0938595867742349], 'output': 0.5318825163197158, 'delta': 0.13242997324294603}, {'weights': [-2.21522690044859, -0.45411859120094666, 0.43276706790505337], 'output': 0.5969121971720253, 'delta': -0.09698616055895373}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_inputs = len(dataset[0]) - 1\n",
    "n_outputs = len(set([row[-1] for row in dataset]))\n",
    "network = initialize_network(n_inputs, 2, n_outputs)\n",
    "train_network(network, dataset, 0.5, 20, n_outputs)\n",
    "\n",
    "for layer in network:\n",
    "    print(layer)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0]\n",
      "[0.8094918973879515, 0.7734292563511262]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\딥러닝입문\\ML_Mastery\\Backpropagation_MyOwn.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9E%85%EB%AC%B8/ML_Mastery/Backpropagation_MyOwn.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m n_outputs \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m([row[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m dataset]))\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9E%85%EB%AC%B8/ML_Mastery/Backpropagation_MyOwn.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m network \u001b[39m=\u001b[39m initialize_network(n_inputs, \u001b[39m2\u001b[39m, n_outputs)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9E%85%EB%AC%B8/ML_Mastery/Backpropagation_MyOwn.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train_network(network, dataset, \u001b[39m0.5\u001b[39;49m, \u001b[39m20\u001b[39;49m, n_outputs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9E%85%EB%AC%B8/ML_Mastery/Backpropagation_MyOwn.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m network:\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9E%85%EB%AC%B8/ML_Mastery/Backpropagation_MyOwn.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(layer)\n",
      "\u001b[1;32me:\\딥러닝입문\\ML_Mastery\\Backpropagation_MyOwn.ipynb Cell 23\u001b[0m in \u001b[0;36mtrain_network\u001b[1;34m(network, train, l_rate, n_epoch, n_outputs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9E%85%EB%AC%B8/ML_Mastery/Backpropagation_MyOwn.ipynb#X33sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(expected)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9E%85%EB%AC%B8/ML_Mastery/Backpropagation_MyOwn.ipynb#X33sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(outputs) \n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9E%85%EB%AC%B8/ML_Mastery/Backpropagation_MyOwn.ipynb#X33sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m sum_error \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39msum\u001b[39m([(expected[i] \u001b[39m-\u001b[39m outputs[\u001b[39m0\u001b[39m][i]) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(expected))])\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9E%85%EB%AC%B8/ML_Mastery/Backpropagation_MyOwn.ipynb#X33sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m backward_propagate_error(network, expected)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9E%85%EB%AC%B8/ML_Mastery/Backpropagation_MyOwn.ipynb#X33sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m update_weights(network, row, l_rate) \n",
      "\u001b[1;32me:\\딥러닝입문\\ML_Mastery\\Backpropagation_MyOwn.ipynb Cell 23\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9E%85%EB%AC%B8/ML_Mastery/Backpropagation_MyOwn.ipynb#X33sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(expected)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9E%85%EB%AC%B8/ML_Mastery/Backpropagation_MyOwn.ipynb#X33sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(outputs) \n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9E%85%EB%AC%B8/ML_Mastery/Backpropagation_MyOwn.ipynb#X33sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m sum_error \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39msum\u001b[39m([(expected[i] \u001b[39m-\u001b[39m outputs[\u001b[39m0\u001b[39;49m][i]) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(expected))])\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9E%85%EB%AC%B8/ML_Mastery/Backpropagation_MyOwn.ipynb#X33sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m backward_propagate_error(network, expected)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9E%85%EB%AC%B8/ML_Mastery/Backpropagation_MyOwn.ipynb#X33sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m update_weights(network, row, l_rate) \n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "n_inputs = len(dataset[0]) - 1\n",
    "n_outputs = len(set([row[-1] for row in dataset]))\n",
    "network = initialize_network(n_inputs, 2, n_outputs)\n",
    "train_network(network, dataset, 0.5, 20, n_outputs)\n",
    "\n",
    "for layer in network:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0]\n",
      "[0, 1, 0]\n",
      "[0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(expected)):\n",
    "    expected = [0 for i in range(3)]\n",
    "    expected[i] = 1\n",
    "    print(expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "outputs = [1,0,2,0,1,5]\n",
    "for output in outputs:\n",
    "    expected = [0 for j in range(len(outputs))]\n",
    "    expected[output] = 1\n",
    "    print(expected)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b18c970f1c267b2ae7de1f51c7b9ea29dc67f6c9f87d5b0ef47c4534e0830b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
