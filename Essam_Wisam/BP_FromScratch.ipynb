{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementing Backpropagation From Scratch on Python 3+\n",
    "- Let's see if theory and practice are the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sympy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide sigmoid and sigmoid_derivative defined function\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide some structure of the network\n",
    "class Network:\n",
    "    def __init__(self, structure):   \n",
    "        # A list that contains the number of neurons in each layer\n",
    "        self.structure = structure\n",
    "        # number of layers in the network\n",
    "        self.num_layers = len(structure)\n",
    "        # A list of all the bias vectors in the network, _n indicates it has all the bias vectors\n",
    "        self.B_n = [np.random.randn(l, 1) for l in structure[1:]]\n",
    "        # A list of all the weight vectors in the network\n",
    "        self.W_n = [np.random.randn(l, next_l) for l, next_l in zip(structure[:-1], structure[1:])]\n",
    "        \n",
    "    # Call forth all the equations for forward and backward process\n",
    "    def backprop(self, x, y):\n",
    "        # Initialize bias vector & weights matrix in the network\n",
    "        e_Je_B_ns = [np.zeros(b.shape) for b in self.B_n]\n",
    "        e_Je_W_ns = [np.zeros(W.shape) for W in self.W_n]\n",
    "        \n",
    "        # Forward pass\n",
    "        # Create two lists that contain all the neuron values before & after activation        \n",
    "        Z_n, A_n = [], []\n",
    "        \n",
    "        # Forward pass layer by layer from L=0 thru L=H\n",
    "        for b, W in zip(self.B_n, self.W_n):\n",
    "            a = x\n",
    "            z = np.dot(W.T, a) + b\n",
    "            a = sigmoid(z)\n",
    "            \n",
    "            Z_n.append(z)\n",
    "            A_n.append(a) \n",
    "            \n",
    "            x = a\n",
    "            \n",
    "        # H : output layer\n",
    "        H = self.num_layers - 2\n",
    "        \n",
    "        # backpropagation\n",
    "        for L in range(H, -1, -1):\n",
    "            if L != H:\n",
    "                delta = np.dot(sigmoid_derivative(Z_n[L]), (self.W_n[L+1]*delta))\n",
    "            else:\n",
    "                delta = np.dot((A_n[L] - y), sigmoid_derivative(Z_n[L]))\n",
    "                \n",
    "            e_Je_B_ns[L] = delta\n",
    "            \n",
    "            if L != 0:\n",
    "                e_Je_W_ns[L] = A_n[L-1] * delta.T    \n",
    "            else:\n",
    "                e_Je_W_ns[L] = x * delta.T \n",
    "        \n",
    "        return e_Je_B_ns, e_Je_W_ns\n",
    "    \n",
    "    # Gradient Descent\n",
    "    def gradient_descent(self, mini_batch, learning_rate):\n",
    "        e_Je_B = [np.zeros(b.shape) for b in self.B_n]\n",
    "        e_Je_W = [np.zeros(W.shape) for W in self.W_n]\n",
    "        \n",
    "        for x, y in mini_batch:\n",
    "            e_Je_B, e_Je_W = self.backprop(x, y)\n",
    "            e_Je_B = []    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0]\n",
      " [3 4]]\n",
      "\n",
      "[[0.73105858 0.5       ]\n",
      " [0.95257413 0.98201379]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,0], [3,4]])\n",
    "y = 1.0 / (1.0 + np.exp(-x))\n",
    "print(x)\n",
    "print()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4*x**3 + 3*x\n",
      "12*x**2 + 3\n",
      "111\n"
     ]
    }
   ],
   "source": [
    "x = symbols(\"x\")\n",
    "fx = 4 * (x**3) + 3 * x\n",
    "print(fx)\n",
    "\n",
    "first_deriv = Derivative(fx, x).doit()\n",
    "print(first_deriv)\n",
    "\n",
    "value = first_deriv.subs({x:3})\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  9, 10])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([2,3,5])\n",
    "y = np.array([5,3,2])\n",
    "\n",
    "x * y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b18c970f1c267b2ae7de1f51c7b9ea29dc67f6c9f87d5b0ef47c4534e0830b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
