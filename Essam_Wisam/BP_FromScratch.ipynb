{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementing Backpropagation From Scratch on Python 3+\n",
    "- Let's see if theory and practice are the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sympy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide sigmoid and sigmoid_derivative defined function\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide some structure of the network\n",
    "class Network:\n",
    "    def __init__(self, structure):   \n",
    "        # A list that contains the number of neurons in each layer\n",
    "        self.structure = structure\n",
    "        # number of layers in the network\n",
    "        self.num_layers = len(structure)\n",
    "        # A list of all the bias vectors in the network, _n indicates it has all the bias vectors\n",
    "        self.B_n = [np.random.randn(l, 1) for l in structure[1:]]\n",
    "        # A list of all the weight vectors in the network\n",
    "        self.W_n = [np.random.randn(l, next_l) for l, next_l in zip(structure[:-1], structure[1:])]\n",
    "        \n",
    "    # Call forth all the equations for forward and backward process\n",
    "    def backprop(self, x, y):\n",
    "        # Initialize bias vector & weights matrix in the network\n",
    "        e_Je_B_ns = [np.zeros(b.shape) for b in self.B_n]\n",
    "        e_Je_W_ns = [np.zeros(W.shape) for W in self.W_n]\n",
    "        \n",
    "        # Forward pass\n",
    "        # Create two lists that contain all the neuron values before & after activation        \n",
    "        Z_n, A_n = [], []\n",
    "        \n",
    "        # Forward pass layer by layer from L=0 thru L=H\n",
    "        for b, W in zip(self.B_n, self.W_n):\n",
    "            a = x\n",
    "            z = np.dot(W.T, a) + b\n",
    "            a = sigmoid(z)\n",
    "            \n",
    "            Z_n.append(z)\n",
    "            A_n.append(a) \n",
    "            \n",
    "            x = a\n",
    "            \n",
    "        # H : output layer\n",
    "        H = self.num_layers - 2\n",
    "        \n",
    "        # backpropagation\n",
    "        for L in range(H, -1, -1):\n",
    "            if L != H:\n",
    "                delta = sigmoid_derivative(Z_n[L]) * np.dot(self.W_n[L+1], delta)\n",
    "            else:\n",
    "                delta = sigmoid_derivative(Z_n[L]) * (A_n[L] - y)\n",
    "                \n",
    "            e_Je_B_ns[L] = delta\n",
    "            \n",
    "            if L != 0:\n",
    "                e_Je_W_ns[L] = A_n[L-1] * delta.T    \n",
    "            else:\n",
    "                e_Je_W_ns[L] = x * delta.T \n",
    "        \n",
    "        return e_Je_B_ns, e_Je_W_ns\n",
    "    \n",
    "    # Gradient Descent\n",
    "    def gradient_descent(self, mini_batch, learning_rate):\n",
    "        e_Je_B_n = [np.zeros(b.shape) for b in self.B_n]\n",
    "        e_Je_W_n = [np.zeros(W.shape) for W in self.W_n]\n",
    "        \n",
    "        for x, y in mini_batch:\n",
    "            e_Je_B_ns, e_Je_W_ns = self.backprop(x, y)\n",
    "            e_Je_B_n = [e_Je_b + e_Je_b_s for e_Je_b, e_Je_b_s in zip(e_Je_B_n, e_Je_B_ns)] \n",
    "            e_Je_W_n = [e_Je_W + e_Je_W_s for e_Je_W, e_Je_W_s in zip(e_Je_W_n, e_Je_W_ns)]\n",
    "            \n",
    "        d = len(mini_batch)\n",
    "        self.W_n = [W - learning_rate/d * e_Je_W for W, e_Je_W in zip(self.W_n, e_Je_W_n)]\n",
    "        self.B_n = [b - learning_rate/d * e_Je_b for b, e_Je_b in zip(self.B_n, e_Je_B_n)]\n",
    "        \n",
    "    def train(self, epochs, training_data, learning_rate):\n",
    "        for j in range(epochs):\n",
    "            for mini_batch in training_data:\n",
    "                self.gradient_descent(mini_batch, learning_rate)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Weights:\n",
      "[[-0.10215984 -1.14129263]\n",
      " [ 2.65440726  1.44060519]\n",
      " [ 0.09890227 -3.12153215]]\n"
     ]
    }
   ],
   "source": [
    "# Insert data\n",
    "np.random.seed(2023)\n",
    "my_net = Network([3, 2, 2])\n",
    "\n",
    "print(\"Initial Weights:\")\n",
    "print(my_net.W_n[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,2) (2,2) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\딥러닝입문\\Essam_Wisam\\BP_FromScratch.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9E%85%EB%AC%B8/Essam_Wisam/BP_FromScratch.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m random_vectors \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m dim, cnt : [np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand(dim, \u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(cnt)]\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9E%85%EB%AC%B8/Essam_Wisam/BP_FromScratch.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m random_batch \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(random_vectors(\u001b[39m3\u001b[39m, \u001b[39m64\u001b[39m), random_vectors(\u001b[39m2\u001b[39m, \u001b[39m64\u001b[39m)))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9E%85%EB%AC%B8/Essam_Wisam/BP_FromScratch.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m my_net\u001b[39m.\u001b[39;49mgradient_descent(random_batch, \u001b[39m3.0\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9E%85%EB%AC%B8/Essam_Wisam/BP_FromScratch.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mOptimized Weights:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9E%85%EB%AC%B8/Essam_Wisam/BP_FromScratch.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(my_net\u001b[39m.\u001b[39mW_n[\u001b[39m0\u001b[39m])\n",
      "\u001b[1;32me:\\딥러닝입문\\Essam_Wisam\\BP_FromScratch.ipynb Cell 6\u001b[0m in \u001b[0;36mNetwork.gradient_descent\u001b[1;34m(self, mini_batch, learning_rate)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9E%85%EB%AC%B8/Essam_Wisam/BP_FromScratch.ipynb#X11sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     e_Je_B_ns, e_Je_W_ns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackprop(x, y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9E%85%EB%AC%B8/Essam_Wisam/BP_FromScratch.ipynb#X11sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     e_Je_B_n \u001b[39m=\u001b[39m [e_Je_b \u001b[39m+\u001b[39m e_Je_b_s \u001b[39mfor\u001b[39;00m e_Je_b, e_Je_b_s \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(e_Je_B_n, e_Je_B_ns)] \n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9E%85%EB%AC%B8/Essam_Wisam/BP_FromScratch.ipynb#X11sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     e_Je_W_n \u001b[39m=\u001b[39m [e_Je_W \u001b[39m+\u001b[39m e_Je_W_s \u001b[39mfor\u001b[39;00m e_Je_W, e_Je_W_s \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(e_Je_W_n, e_Je_W_ns)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9E%85%EB%AC%B8/Essam_Wisam/BP_FromScratch.ipynb#X11sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m d \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(mini_batch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9E%85%EB%AC%B8/Essam_Wisam/BP_FromScratch.ipynb#X11sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_n \u001b[39m=\u001b[39m [W \u001b[39m-\u001b[39m learning_rate\u001b[39m/\u001b[39md \u001b[39m*\u001b[39m e_Je_W \u001b[39mfor\u001b[39;00m W, e_Je_W \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_n, e_Je_W_n)]\n",
      "\u001b[1;32me:\\딥러닝입문\\Essam_Wisam\\BP_FromScratch.ipynb Cell 6\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9E%85%EB%AC%B8/Essam_Wisam/BP_FromScratch.ipynb#X11sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     e_Je_B_ns, e_Je_W_ns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackprop(x, y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9E%85%EB%AC%B8/Essam_Wisam/BP_FromScratch.ipynb#X11sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     e_Je_B_n \u001b[39m=\u001b[39m [e_Je_b \u001b[39m+\u001b[39m e_Je_b_s \u001b[39mfor\u001b[39;00m e_Je_b, e_Je_b_s \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(e_Je_B_n, e_Je_B_ns)] \n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9E%85%EB%AC%B8/Essam_Wisam/BP_FromScratch.ipynb#X11sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     e_Je_W_n \u001b[39m=\u001b[39m [e_Je_W \u001b[39m+\u001b[39;49m e_Je_W_s \u001b[39mfor\u001b[39;00m e_Je_W, e_Je_W_s \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(e_Je_W_n, e_Je_W_ns)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9E%85%EB%AC%B8/Essam_Wisam/BP_FromScratch.ipynb#X11sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m d \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(mini_batch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9E%85%EB%AC%B8/Essam_Wisam/BP_FromScratch.ipynb#X11sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_n \u001b[39m=\u001b[39m [W \u001b[39m-\u001b[39m learning_rate\u001b[39m/\u001b[39md \u001b[39m*\u001b[39m e_Je_W \u001b[39mfor\u001b[39;00m W, e_Je_W \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_n, e_Je_W_n)]\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,2) (2,2) "
     ]
    }
   ],
   "source": [
    "# The following generates a list of cnt vectors of length dim.\n",
    "np.random.seed(2023)\n",
    "random_vectors = lambda dim, cnt : [np.random.rand(dim, 1) for i in range(cnt)]\n",
    "random_batch = list(zip(random_vectors(3, 64), random_vectors(2, 64)))\n",
    "\n",
    "my_net.gradient_descent(random_batch, 3.0)\n",
    "print(\"Optimized Weights:\")\n",
    "print(my_net.W_n[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.37214444]\n",
      " [0.57783618]\n",
      " [0.01429624]]\n",
      "\n",
      "[[0.92490225]\n",
      " [0.49228575]\n",
      " [0.66290319]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for rand_vector in random_vectors(3, 64)[-2:]:\n",
    "    print(rand_vector)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0]\n",
      " [3 4]]\n",
      "\n",
      "[[0.73105858 0.5       ]\n",
      " [0.95257413 0.98201379]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,0], [3,4]])\n",
    "y = 1.0 / (1.0 + np.exp(-x))\n",
    "print(x)\n",
    "print()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4*x**3 + 3*x\n",
      "12*x**2 + 3\n",
      "111\n"
     ]
    }
   ],
   "source": [
    "x = symbols(\"x\")\n",
    "fx = 4 * (x**3) + 3 * x\n",
    "print(fx)\n",
    "\n",
    "first_deriv = Derivative(fx, x).doit()\n",
    "print(first_deriv)\n",
    "\n",
    "value = first_deriv.subs({x:3})\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  9, 10])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([2,3,5])\n",
    "y = np.array([5,3,2])\n",
    "\n",
    "x * y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b18c970f1c267b2ae7de1f51c7b9ea29dc67f6c9f87d5b0ef47c4534e0830b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
