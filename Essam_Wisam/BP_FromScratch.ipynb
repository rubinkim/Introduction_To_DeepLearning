{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementing Backpropagation From Scratch on Python 3+\n",
    "- Let's see if theory and practice are the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sympy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide sigmoid and sigmoid_derivative defined function\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide some structure of the network\n",
    "class Network:\n",
    "    def __init__(self, structure):   \n",
    "        # A list that contains the number of neurons in each layer of the network\n",
    "        self.structure = structure\n",
    "        # Number of layers in the network\n",
    "        self.num_layers = len(structure)\n",
    "        # A list of all the bias vectors in the network, _n indicates it has all the bias vectors\n",
    "        self.B_n = [np.random.randn(l, 1) for l in structure[1:]]\n",
    "        # A list of all the weight vectors in the network\n",
    "        self.W_n = [np.random.randn(l, next_l) for l, next_l in zip(structure[:-1], structure[1:])]\n",
    "        \n",
    "    # Call forth all the equations for forward and backward process\n",
    "    def backprop(self, x, y):\n",
    "        # Initialize bias vector & weights matrix of each layer in the network\n",
    "        e_Je_B_ns = [np.zeros(b.shape) for b in self.B_n]\n",
    "        e_Je_W_ns = [np.zeros(W.shape) for W in self.W_n]\n",
    "        \n",
    "        # Forward pass\n",
    "        # Create two lists that contain all the neuron values before & after activation        \n",
    "        Z_n, A_n = [], []\n",
    "        \n",
    "        # Forward pass layer by layer from L=0 thru L=H\n",
    "        for i, (b, W) in enumerate(zip(self.B_n, self.W_n)):\n",
    "            if i == 0:           \n",
    "                z = np.dot(W.T, x) + b\n",
    "            elif i >= 1:\n",
    "                z = np.dot(W.T, a) + b\n",
    "            a = sigmoid(z)    \n",
    "            \n",
    "            Z_n.append(z)\n",
    "            A_n.append(a)             \n",
    "                      \n",
    "            \n",
    "        # H : output layer\n",
    "        H = self.num_layers - 2\n",
    "        \n",
    "        # backpropagation\n",
    "        for L in range(H, -1, -1):\n",
    "            if L != H:\n",
    "                delta = sigmoid_derivative(Z_n[L]) * np.dot(self.W_n[L+1], delta)\n",
    "            else:\n",
    "                delta = sigmoid_derivative(Z_n[L]) * (A_n[L] - y)\n",
    "                \n",
    "            e_Je_B_ns[L] = delta\n",
    "            \n",
    "            if L != 0:\n",
    "                e_Je_W_ns[L] = np.dot(A_n[L-1], delta.T)    \n",
    "            else:\n",
    "                e_Je_W_ns[L] = np.dot(x, delta.T) \n",
    "        \n",
    "        return e_Je_B_ns, e_Je_W_ns\n",
    "    \n",
    "    # Gradient Descent\n",
    "    def gradient_descent(self, mini_batch, learning_rate):\n",
    "        # Initialize bias vector & weights matrix of each layer in the network\n",
    "        e_Je_B_n = [np.zeros(b.shape) for b in self.B_n]\n",
    "        e_Je_W_n = [np.zeros(W.shape) for W in self.W_n]\n",
    "        \n",
    "        for x, y in mini_batch:\n",
    "            e_Je_B_ns, e_Je_W_ns = self.backprop(x, y)\n",
    "            e_Je_B_n = [e_Je_b + e_Je_b_s for e_Je_b, e_Je_b_s in zip(e_Je_B_n, e_Je_B_ns)] \n",
    "            e_Je_W_n = [e_Je_W + e_Je_W_s for e_Je_W, e_Je_W_s in zip(e_Je_W_n, e_Je_W_ns)]\n",
    "            \n",
    "        d = len(mini_batch)\n",
    "        self.W_n = [W - learning_rate/d * e_Je_W for W, e_Je_W in zip(self.W_n, e_Je_W_n)]\n",
    "        self.B_n = [b - learning_rate/d * e_Je_b for b, e_Je_b in zip(self.B_n, e_Je_B_n)]\n",
    "        \n",
    "    def train(self, epochs, training_data, learning_rate):\n",
    "        for j in range(epochs):\n",
    "            for mini_batch in training_data:\n",
    "                self.gradient_descent(mini_batch, learning_rate)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Weights:\n",
      "[[-1.14129263  2.65440726  1.44060519]\n",
      " [ 0.09890227 -3.12153215 -1.07652165]\n",
      " [-0.32568196 -1.03549788 -0.42632038]\n",
      " [-1.0293614  -0.5217742  -0.42275757]]\n",
      "\n",
      "[[-0.09924417 -1.38025801]\n",
      " [ 0.30190282 -1.03359932]\n",
      " [-1.50217752 -1.3795934 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Insert data\n",
    "np.random.seed(2023)\n",
    "my_net = Network([4, 3, 2])\n",
    "\n",
    "print(\"Initial Weights:\")\n",
    "\n",
    "for weights in my_net.W_n:\n",
    "    print(weights)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0.3219883 ],\n",
      "       [0.89042245],\n",
      "       [0.58805226],\n",
      "       [0.12659609]]), array([[0.52438734],\n",
      "       [0.54493524]]))\n",
      "\n",
      "(array([[0.14134122],\n",
      "       [0.46789559],\n",
      "       [0.02208966],\n",
      "       [0.72727471]]), array([[0.45637326],\n",
      "       [0.50138226]]))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2023)\n",
    "random_vectors = lambda dim, cnt : [np.random.rand(dim, 1) for i in range(cnt)]\n",
    "a = list(zip(random_vectors(4, 2), random_vectors(2, 2)))\n",
    "for i in a:\n",
    "    print(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.01914025],\n",
      "       [0.00265512],\n",
      "       [0.01052472]]), array([[-0.00695039],\n",
      "       [-0.05513812]])]\n",
      "\n",
      "[array([[0.00616294, 0.00085492, 0.00338884],\n",
      "       [0.01704291, 0.00236418, 0.00937144],\n",
      "       [0.01125547, 0.00156135, 0.00618908],\n",
      "       [0.00242308, 0.00033613, 0.00133239]]), array([[-0.00366687, -0.02908966],\n",
      "       [-0.00035424, -0.00281024],\n",
      "       [-0.00098525, -0.0078161 ]])]\n",
      "\n",
      "\n",
      "[array([[0.01596324],\n",
      "       [0.00625771],\n",
      "       [0.00693993]]), array([[ 0.00993982],\n",
      "       [-0.04727001]])]\n",
      "\n",
      "[array([[0.00225626, 0.00088447, 0.0009809 ],\n",
      "       [0.00746913, 0.00292795, 0.00324716],\n",
      "       [0.00035262, 0.00013823, 0.0001533 ],\n",
      "       [0.01160966, 0.00455107, 0.00504724]]), array([[ 0.00457518, -0.0217578 ],\n",
      "       [ 0.00139527, -0.00663537],\n",
      "       [ 0.00164371, -0.00781685]])]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x, y in a:\n",
    "    xx, yy = my_net.backprop(x, y)\n",
    "    print(xx)\n",
    "    print()\n",
    "    print(yy)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_net.gradient_descent(a, 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Weights:\n",
      "Weights matrix\n",
      "[[-1.18750966  2.62524535  1.40807124]\n",
      " [ 0.04163495 -3.14496124 -1.11252054]\n",
      " [-0.37555453 -1.06143581 -0.45080004]\n",
      " [-1.074681   -0.54867962 -0.44455736]]\n",
      "\n",
      "Bias vector\n",
      "[[ 0.59668323]\n",
      " [-0.38485834]\n",
      " [-1.07019562]]\n",
      "\n",
      "\n",
      "Weights matrix\n",
      "[[-0.12507016 -1.22419588]\n",
      " [ 0.28344259 -0.94614776]\n",
      " [-1.51648692 -1.2977088 ]]\n",
      "\n",
      "Bias vector\n",
      "[[0.15872406]\n",
      " [0.2731806 ]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The following generates a list of cnt vectors of length dim.\n",
    "np.random.seed(2023)\n",
    "random_vectors = lambda dim, cnt : [np.random.rand(dim, 1) for i in range(cnt)]\n",
    "random_batch = list(zip(random_vectors(4, 64), random_vectors(2, 64)))\n",
    "\n",
    "my_net.gradient_descent(random_batch, 3.0)\n",
    "print(\"Optimized Weights:\")\n",
    "\n",
    "for weight, bias in zip(my_net.W_n, my_net.B_n):\n",
    "    print(\"Weights matrix\")\n",
    "    print(weight)\n",
    "    print()\n",
    "    print(\"Bias vector\")\n",
    "    print(bias)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.37214444]\n",
      " [0.57783618]\n",
      " [0.01429624]]\n",
      "\n",
      "[[0.92490225]\n",
      " [0.49228575]\n",
      " [0.66290319]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for rand_vector in random_vectors(3, 64)[-2:]:\n",
    "    print(rand_vector)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0]\n",
      " [3 4]]\n",
      "\n",
      "[[0.73105858 0.5       ]\n",
      " [0.95257413 0.98201379]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,0], [3,4]])\n",
    "y = 1.0 / (1.0 + np.exp(-x))\n",
    "print(x)\n",
    "print()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4*x**3 + 3*x\n",
      "12*x**2 + 3\n",
      "111\n"
     ]
    }
   ],
   "source": [
    "x = symbols(\"x\")\n",
    "fx = 4 * (x**3) + 3 * x\n",
    "print(fx)\n",
    "\n",
    "first_deriv = Derivative(fx, x).doit()\n",
    "print(first_deriv)\n",
    "\n",
    "value = first_deriv.subs({x:3})\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  9, 10])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([2,3,5])\n",
    "y = np.array([5,3,2])\n",
    "\n",
    "x * y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b18c970f1c267b2ae7de1f51c7b9ea29dc67f6c9f87d5b0ef47c4534e0830b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
