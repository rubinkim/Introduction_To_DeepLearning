{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 2, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# Construct a list that contains the number of neurons of each layer in your network.\n",
    "structure = input(\"Enter the number of neurons of each layer in your network\").split(' ')\n",
    "structure = [int(x) for x in structure]\n",
    "print(structure)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias vector of each layer\n",
      "\n",
      "[[-1.0856306 ]\n",
      " [ 0.99734545]]\n",
      "\n",
      "[[ 0.2829785 ]\n",
      " [-1.50629471]]\n",
      "\n",
      "[[-0.57860025]\n",
      " [ 1.65143654]\n",
      " [-2.42667924]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Construct a list that contains all the bias vectors of your network(_n means it has all vectors)\n",
    "np.random.seed(123)\n",
    "B_n = [np.random.randn(l, 1) for l in structure[1:]]\n",
    "print(\"bias vector of each layer\\n\")\n",
    "for B in B_n:\n",
    "    print(B)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight matrix of each layer\n",
      "\n",
      "[[-1.0856306   0.99734545]\n",
      " [ 0.2829785  -1.50629471]\n",
      " [-0.57860025  1.65143654]\n",
      " [-2.42667924 -0.42891263]]\n",
      "\n",
      "[[ 1.26593626 -0.8667404 ]\n",
      " [-0.67888615 -0.09470897]]\n",
      "\n",
      "[[ 1.49138963 -0.638902   -0.44398196]\n",
      " [-0.43435128  2.20593008  2.18678609]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Construct a list that contains all the weight matrices of your network(_n means it has all matrices)\n",
    "np.random.seed(123)\n",
    "W_n = [np.random.randn(l, next_l) for l, next_l in zip(structure[:-1], structure[1:])]\n",
    "print(\"weight matrix of each layer\\n\")\n",
    "for W in W_n:\n",
    "    print(W)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_iris().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(150, 4) (150,)\n",
      "150 150\n"
     ]
    }
   ],
   "source": [
    "data = load_iris()['data']\n",
    "target = load_iris()['target']\n",
    "\n",
    "print(type(data), type(target))\n",
    "print(data.shape, target.shape)\n",
    "print(len(data), len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[134, 74, 108, 47, 8, 18, 121, 36, 118, 60, 97, 42, 66, 43, 106, 133, 92, 62, 26, 11, 122, 52, 109, 55, 84, 107, 17, 129, 105, 16, 100, 143, 49, 78, 57, 142, 149, 14, 30, 75, 144, 1, 102, 3, 104, 139, 56, 13, 58, 90, 87, 28, 119, 128, 147, 82, 132, 15, 48, 117, 10, 44, 148, 65, 50, 101, 61, 140, 7, 95, 126, 91, 96, 12, 4, 115, 9, 79, 88, 46, 138, 141, 64, 37, 131, 35, 2, 98, 72, 83, 116, 24, 19, 0, 38, 86, 33, 63, 112, 39, 85, 94, 123, 54, 76, 59, 130, 67, 40, 111, 25, 99, 81, 89, 51, 71, 5, 70, 29, 124, 68, 73, 93, 23, 41, 125, 22, 77, 34, 135, 53, 146, 114, 31, 110, 20, 21, 45, 27, 103, 137, 113, 120, 145, 127, 6, 69, 80, 136, 32]\n"
     ]
    }
   ],
   "source": [
    "ind = np.arange(150)\n",
    "np.random.shuffle(ind)\n",
    "indexes = ind.tolist()\n",
    "print(type(indexes))\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.7 3.1 5.6 2.4]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [6.7 3.  5.  1.7]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [6.  3.  4.8 1.8]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [6.  2.2 5.  1.5]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [4.4 2.9 1.4 0.2]]\n",
      "\n",
      "[2 2 0 0 1 1 2 0 1 0 1 0 1 2 2 0 0 2 0 1 0 2 1 1 1 2 2 2 2 0 2 1 1 1 0 2 2\n",
      " 0 2 0 0 2 1 0 1 0 2 0 1 2 0 2 0 1 1 2 1 1 2 2 0 1 2 0 0 2 0 0 0 0 2 1 2 1\n",
      " 2 2 0 2 1 1 2 2 1 0 2 1 1 2 1 0 1 2 2 1 0 0 2 2 1 0 1 2 1 2 2 0 2 0 1 0 0\n",
      " 1 0 0 1 1 1 1 0 2 1 0 1 1 0 1 0 2 2 2 2 1 2 0 1 2 0 1 2 1 1 0 2 0 1 0 0 1\n",
      " 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Just pick up 5 indexes out of dataset randomly\n",
    "ind = np.arange(150)\n",
    "np.random.shuffle(ind)\n",
    "indexes = ind.tolist()\n",
    "\n",
    "data = data[indexes]\n",
    "target = target[indexes]\n",
    "\n",
    "print(data)\n",
    "print()\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.7 3.1 5.6 2.4]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [6.7 3.  5.  1.7]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [6.  3.  4.8 1.8]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [6.  2.2 5.  1.5]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [4.4 2.9 1.4 0.2]]\n",
      "\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X = data\n",
    "print(X)\n",
    "print()\n",
    "\n",
    "Y = np.zeros((len(X), 3))\n",
    "for i in range(len(Y)):\n",
    "    for j in range(len(Y[i])):\n",
    "        Y[i, target[i]] = 1\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B_n is a list of 3 np.ndarrays with (2, 1),(2, 1),(3, 1)\n",
    "# W_n is a list of 3 np.ndarrays with (4, 2),(2, 2),(2, 3)\n",
    "\n",
    "# X is a np.ndarray with shape (150,4)\n",
    "# Y is a np.ndarray with shape (150,3)\n",
    "\n",
    "# Z_n is a list of 3 np.ndarrays with (2,1),(2,1),(3,1)\n",
    "# A_n is a list of 3 np.ndarrays with (2,1),(2,1),(3,1)\n",
    "\n",
    "# e_Je_B_ns a list of 3 np.ndarrays with (2,1), (2,1), (3,1)\n",
    "# e_Je_W_ns a list of 3 np.ndarrays with (4,2), (2,2), (2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide sigmoid and sigmoid_derivative function\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Loss\n",
    "def calculate_loss(epoch, actual, expected):\n",
    "    diff = np.sum(np.sqrt((actual.reshape(-1) - expected) * (actual.reshape(-1) - expected)))\n",
    "    print(f\"{epoch}th error : {diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.3 -0.2  0.1]\n",
      "[0.09 0.04 0.01]\n",
      "[0.3 0.2 0.1]\n",
      "0.6\n",
      "0.6\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[0.3],[0.8],[0.1]])\n",
    "y = [0, 1, 0]\n",
    "\n",
    "#print(calculate_loss(a, y))\n",
    "print((a.reshape(-1) - y)) \n",
    "print((a.reshape(-1) - y) * (a.reshape(-1) - y)) \n",
    "print(np.sqrt((a.reshape(-1) - y) * (a.reshape(-1) - y)))\n",
    "print(np.sum(np.sqrt((a.reshape(-1) - y) * (a.reshape(-1) - y))))\n",
    "\n",
    "print(calculate_loss(a, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th error : 1.901916429358297\n",
      "1th error : 2.103646436076745\n",
      "2th error : 1.5096268401324056\n",
      "3th error : 1.3163526642150227\n",
      "4th error : 0.9924666516699315\n",
      "5th error : 2.1684321435095715\n",
      "6th error : 2.105756219131735\n",
      "7th error : 2.1926045408028614\n",
      "8th error : 1.9910709941855262\n",
      "9th error : 1.3463485278432368\n",
      "10th error : 0.9560642297364496\n",
      "11th error : 1.5776704235428842\n",
      "12th error : 2.064274094295981\n",
      "13th error : 2.0845758623199395\n",
      "14th error : 1.8364416993599935\n",
      "15th error : 1.521208676543188\n",
      "16th error : 0.5913249385283628\n",
      "17th error : 1.8259454227749319\n",
      "18th error : 1.6417100023607611\n",
      "19th error : 1.6951152804524094\n",
      "20th error : 1.3919173005907197\n",
      "21th error : 2.0687746119761936\n",
      "22th error : 1.5533274242663113\n",
      "23th error : 1.7239184020126856\n",
      "24th error : 1.8462665589293012\n",
      "25th error : 1.4865026605071805\n",
      "26th error : 1.276289708382821\n",
      "27th error : 1.7009349708715926\n",
      "28th error : 1.7244269714078522\n",
      "29th error : 1.5724328053579681\n",
      "30th error : 2.101728221753402\n",
      "31th error : 1.2711229897728273\n",
      "32th error : 0.7843392944729972\n",
      "33th error : 2.5001943190036857\n",
      "34th error : 2.1226005164903237\n",
      "35th error : 1.744483857112408\n",
      "36th error : 1.0089614394866664\n",
      "37th error : 0.8177268586185322\n",
      "38th error : 1.5616709775399489\n",
      "39th error : 1.7679296878666573\n",
      "40th error : 1.328829466434926\n",
      "41th error : 2.2570591906475856\n",
      "42th error : 1.683579160576591\n",
      "43th error : 0.8713018908073609\n",
      "44th error : 1.5969765499523583\n",
      "45th error : 0.44253032286470145\n",
      "46th error : 1.1811641560959172\n",
      "47th error : 1.620947950793367\n",
      "48th error : 2.21669537814341\n",
      "49th error : 1.9417871013516983\n"
     ]
    }
   ],
   "source": [
    "# Set the output layer to H and let L begin from the first hidden layer(not from the input layer)\n",
    "H = len(structure) - 2\n",
    "learning_rate = 0.05\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for x, y in zip(X, Y):     # x의 shape:(4,),  y의 shape:(3,)\n",
    "        \n",
    "        \n",
    "        W_n = [np.random.randn(l, next_l) for l, next_l in zip(structure[:-1], structure[1:])]\n",
    "        \n",
    "        B_n = [np.random.randn(l, 1) for l in structure[1:]]\n",
    "\n",
    "        \n",
    "        # Forward Propagate\n",
    "        # Create two lists that contain pre and post activation vector of each layer, respectively\n",
    "        Z_n, A_n = [], []\n",
    "        \n",
    "        for i, (b, W) in enumerate(zip(B_n, W_n)):\n",
    "            if i == 0:\n",
    "                z = np.dot(np.array(W).T, x).reshape(-1, 1) + np.array(b)\n",
    "            else:\n",
    "                z = np.dot(np.array(W).T, a).reshape(-1, 1) + np.array(b)\n",
    "            a = sigmoid(z)\n",
    "\n",
    "            Z_n.append(z)\n",
    "            A_n.append(a)\n",
    "        # print(Z_n) \n",
    "        # print()  \n",
    "    \n",
    "    \n",
    "        # Backpropagate\n",
    "        # Initialize a list called e_Je_W_ns that will contain e_Je_W matrices for each layer\n",
    "        e_Je_W_ns = [np.zeros(W.shape) for W in W_n]    # (4, 2), (2, 2), (2, 3)\n",
    "        #for x in e_Je_W_ns:\n",
    "        #    print(x.shape)\n",
    "        #print()\n",
    "\n",
    "        # Initialize a list called e_Je_B_ns that will contain e_Je_B vectors for each layer\n",
    "        e_Je_B_ns = [np.zeros(B.shape) for B in B_n]    # (2, 1), (2, 1), (3, 1)\n",
    "        \n",
    "        for L in range(H, -1, -1):\n",
    "            if L != H:\n",
    "                delta = sigmoid_derivative(Z_n[L]) * np.dot(W_n[L+1], delta)\n",
    "            else:\n",
    "                delta = sigmoid_derivative(Z_n[L]) * (A_n[L] - y.reshape(-1, 1))\n",
    "                \n",
    "            e_Je_B_ns[L] = delta\n",
    "            # print(f\"{L} : {delta}\")\n",
    "            \n",
    "            if L != 0:\n",
    "                e_Je_W_ns[L] = np.dot(A_n[L-1], delta.T)\n",
    "            else:\n",
    "                e_Je_W_ns[L] = np.dot(x.reshape(-1, 1), delta.T)\n",
    "    \n",
    "        \"\"\"\n",
    "        for x in e_Je_W_ns:\n",
    "            print(x)\n",
    "        print()\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        for i, (wn, ejew, bn, ejeb) in enumerate(zip(W_n, e_Je_W_ns, B_n, e_Je_B_ns)):\n",
    "            W_n[i] -= learning_rate/len(X) * ejew\n",
    "            B_n[i] -= learning_rate/len(X) * ejeb\n",
    "            \"\"\"\n",
    "            print(wn)\n",
    "            print(ejew)\n",
    "            print(bn)\n",
    "            print(ejeb)\n",
    "            print()\n",
    "            \"\"\"\n",
    "    \n",
    "    \n",
    "    calculate_loss(epoch, A_n[-1], y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b18c970f1c267b2ae7de1f51c7b9ea29dc67f6c9f87d5b0ef47c4534e0830b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
