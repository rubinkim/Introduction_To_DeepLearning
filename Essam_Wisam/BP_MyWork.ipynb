{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 2, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# Construct a list that contains the number of neurons of each layer in your network.\n",
    "structure = input(\"Enter the number of neurons of each layer in your network\").split(' ')\n",
    "structure = [int(x) for x in structure]\n",
    "print(structure)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias vector of each layer\n",
      "\n",
      "[[-1.0856306 ]\n",
      " [ 0.99734545]]\n",
      "\n",
      "[[ 0.2829785 ]\n",
      " [-1.50629471]]\n",
      "\n",
      "[[-0.57860025]\n",
      " [ 1.65143654]\n",
      " [-2.42667924]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Construct a list that contains all the bias vectors of your network(_n means it has all vectors)\n",
    "np.random.seed(123)\n",
    "B_n = [np.random.randn(l, 1) for l in structure[1:]]\n",
    "print(\"bias vector of each layer\\n\")\n",
    "for B in B_n:\n",
    "    print(B)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight matrix of each layer\n",
      "\n",
      "[[-1.0856306   0.99734545]\n",
      " [ 0.2829785  -1.50629471]\n",
      " [-0.57860025  1.65143654]\n",
      " [-2.42667924 -0.42891263]]\n",
      "\n",
      "[[ 1.26593626 -0.8667404 ]\n",
      " [-0.67888615 -0.09470897]]\n",
      "\n",
      "[[ 1.49138963 -0.638902   -0.44398196]\n",
      " [-0.43435128  2.20593008  2.18678609]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Construct a list that contains all the weight matrices of your network(_n means it has all matrices)\n",
    "np.random.seed(123)\n",
    "W_n = [np.random.randn(l, next_l) for l, next_l in zip(structure[:-1], structure[1:])]\n",
    "print(\"weight matrix of each layer\\n\")\n",
    "for W in W_n:\n",
    "    print(W)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_iris().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(150, 4) (150,)\n",
      "150 150\n"
     ]
    }
   ],
   "source": [
    "data = load_iris()['data']\n",
    "target = load_iris()['target']\n",
    "\n",
    "print(type(data), type(target))\n",
    "print(data.shape, target.shape)\n",
    "print(len(data), len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[134, 74, 108, 47, 8, 18, 121, 36, 118, 60, 97, 42, 66, 43, 106, 133, 92, 62, 26, 11, 122, 52, 109, 55, 84, 107, 17, 129, 105, 16, 100, 143, 49, 78, 57, 142, 149, 14, 30, 75, 144, 1, 102, 3, 104, 139, 56, 13, 58, 90, 87, 28, 119, 128, 147, 82, 132, 15, 48, 117, 10, 44, 148, 65, 50, 101, 61, 140, 7, 95, 126, 91, 96, 12, 4, 115, 9, 79, 88, 46, 138, 141, 64, 37, 131, 35, 2, 98, 72, 83, 116, 24, 19, 0, 38, 86, 33, 63, 112, 39, 85, 94, 123, 54, 76, 59, 130, 67, 40, 111, 25, 99, 81, 89, 51, 71, 5, 70, 29, 124, 68, 73, 93, 23, 41, 125, 22, 77, 34, 135, 53, 146, 114, 31, 110, 20, 21, 45, 27, 103, 137, 113, 120, 145, 127, 6, 69, 80, 136, 32]\n"
     ]
    }
   ],
   "source": [
    "ind = np.arange(150)\n",
    "np.random.shuffle(ind)\n",
    "indexes = ind.tolist()\n",
    "print(type(indexes))\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.7 3.1 5.6 2.4]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [6.7 3.  5.  1.7]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [6.  3.  4.8 1.8]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [6.  2.2 5.  1.5]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [4.4 2.9 1.4 0.2]]\n",
      "\n",
      "[2 2 0 0 1 1 2 0 1 0 1 0 1 2 2 0 0 2 0 1 0 2 1 1 1 2 2 2 2 0 2 1 1 1 0 2 2\n",
      " 0 2 0 0 2 1 0 1 0 2 0 1 2 0 2 0 1 1 2 1 1 2 2 0 1 2 0 0 2 0 0 0 0 2 1 2 1\n",
      " 2 2 0 2 1 1 2 2 1 0 2 1 1 2 1 0 1 2 2 1 0 0 2 2 1 0 1 2 1 2 2 0 2 0 1 0 0\n",
      " 1 0 0 1 1 1 1 0 2 1 0 1 1 0 1 0 2 2 2 2 1 2 0 1 2 0 1 2 1 1 0 2 0 1 0 0 1\n",
      " 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Just pick up 5 indexes out of dataset randomly\n",
    "ind = np.arange(150)\n",
    "np.random.shuffle(ind)\n",
    "indexes = ind.tolist()\n",
    "\n",
    "data = data[indexes]\n",
    "target = target[indexes]\n",
    "\n",
    "print(data)\n",
    "print()\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.7 3.1 5.6 2.4]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [6.7 3.  5.  1.7]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [6.  3.  4.8 1.8]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [6.  2.2 5.  1.5]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [4.4 2.9 1.4 0.2]]\n",
      "\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X = data\n",
    "print(X)\n",
    "print()\n",
    "\n",
    "Y = np.zeros((len(X), 3))\n",
    "for i in range(len(Y)):\n",
    "    for j in range(len(Y[i])):\n",
    "        Y[i, target[i]] = 1\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B_n is a list of 3 np.ndarrays with (2, 1),(2, 1),(3, 1)\n",
    "# W_n is a list of 3 np.ndarrays with (4, 2),(2, 2),(2, 3)\n",
    "\n",
    "# X is a np.ndarray with shape (150,4)\n",
    "# Y is a np.ndarray with shape (150,3)\n",
    "\n",
    "# Z_n is a list of 3 np.ndarrays with (2,1),(2,1),(3,1)\n",
    "# A_n is a list of 3 np.ndarrays with (2,1),(2,1),(3,1)\n",
    "\n",
    "# e_Je_B_ns a list of 3 np.ndarrays with (2,1), (2,1), (3,1)\n",
    "# e_Je_W_ns a list of 3 np.ndarrays with (4,2), (2,2), (2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide sigmoid and sigmoid_derivative function\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Loss\n",
    "def calculate_loss(actual, expected):\n",
    "    diff = np.sum(np.sqrt((actual.reshape(-1) - expected) * (actual.reshape(-1) - expected)))\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.3 -0.2  0.1]\n",
      "[0.09 0.04 0.01]\n",
      "[0.3 0.2 0.1]\n",
      "0.6\n",
      "2th error : 0.6\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[0.3],[0.8],[0.1]])\n",
    "y = [0, 1, 0]\n",
    "\n",
    "#print(calculate_loss(a, y))\n",
    "print((a.reshape(-1) - y)) \n",
    "print((a.reshape(-1) - y) * (a.reshape(-1) - y)) \n",
    "print(np.sqrt((a.reshape(-1) - y) * (a.reshape(-1) - y)))\n",
    "print(np.sum(np.sqrt((a.reshape(-1) - y) * (a.reshape(-1) - y))))\n",
    "\n",
    "print(calculate_loss(2, a, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [2,4,6]\n",
    "np.mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th total error : 224.151\n",
      "1th total error : 223.023\n",
      "2th total error : 220.857\n",
      "3th total error : 224.984\n",
      "4th total error : 217.919\n",
      "5th total error : 228.427\n",
      "6th total error : 228.201\n",
      "7th total error : 219.927\n",
      "8th total error : 233.378\n",
      "9th total error : 218.491\n",
      "10th total error : 222.093\n",
      "11th total error : 228.262\n",
      "12th total error : 229.373\n",
      "13th total error : 231.788\n",
      "14th total error : 229.173\n",
      "15th total error : 215.748\n",
      "16th total error : 227.238\n",
      "17th total error : 227.637\n",
      "18th total error : 236.855\n",
      "19th total error : 214.158\n",
      "20th total error : 227.024\n",
      "21th total error : 219.986\n",
      "22th total error : 213.921\n",
      "23th total error : 227.886\n",
      "24th total error : 223.165\n",
      "25th total error : 226.449\n",
      "26th total error : 220.325\n",
      "27th total error : 229.761\n",
      "28th total error : 216.462\n",
      "29th total error : 224.573\n",
      "30th total error : 228.151\n",
      "31th total error : 222.423\n",
      "32th total error : 230.464\n",
      "33th total error : 231.018\n",
      "34th total error : 222.587\n",
      "35th total error : 218.784\n",
      "36th total error : 223.287\n",
      "37th total error : 234.409\n",
      "38th total error : 226.604\n",
      "39th total error : 221.540\n",
      "40th total error : 235.009\n",
      "41th total error : 221.553\n",
      "42th total error : 229.364\n",
      "43th total error : 224.455\n",
      "44th total error : 229.564\n",
      "45th total error : 233.618\n",
      "46th total error : 220.549\n",
      "47th total error : 218.310\n",
      "48th total error : 225.652\n",
      "49th total error : 239.557\n",
      "50th total error : 217.183\n",
      "51th total error : 222.662\n",
      "52th total error : 224.331\n",
      "53th total error : 217.894\n",
      "54th total error : 226.809\n",
      "55th total error : 230.330\n",
      "56th total error : 220.518\n",
      "57th total error : 225.205\n",
      "58th total error : 221.368\n",
      "59th total error : 215.159\n",
      "60th total error : 223.550\n",
      "61th total error : 229.970\n",
      "62th total error : 223.203\n",
      "63th total error : 220.718\n",
      "64th total error : 226.445\n",
      "65th total error : 217.692\n",
      "66th total error : 230.870\n",
      "67th total error : 225.549\n",
      "68th total error : 222.061\n",
      "69th total error : 223.577\n",
      "70th total error : 217.912\n",
      "71th total error : 226.113\n",
      "72th total error : 222.160\n",
      "73th total error : 233.683\n",
      "74th total error : 221.835\n",
      "75th total error : 222.328\n",
      "76th total error : 222.498\n",
      "77th total error : 218.479\n",
      "78th total error : 226.347\n",
      "79th total error : 227.184\n",
      "80th total error : 222.634\n",
      "81th total error : 222.196\n",
      "82th total error : 223.116\n",
      "83th total error : 222.945\n",
      "84th total error : 229.249\n",
      "85th total error : 221.796\n",
      "86th total error : 228.999\n",
      "87th total error : 225.979\n",
      "88th total error : 227.480\n",
      "89th total error : 225.897\n",
      "90th total error : 228.274\n",
      "91th total error : 224.253\n",
      "92th total error : 224.764\n",
      "93th total error : 232.481\n",
      "94th total error : 234.021\n",
      "95th total error : 228.087\n",
      "96th total error : 228.328\n",
      "97th total error : 226.746\n",
      "98th total error : 229.100\n",
      "99th total error : 232.304\n"
     ]
    }
   ],
   "source": [
    "# Set the output layer to H and let L begin from the first hidden layer(not from the input layer)\n",
    "H = len(structure) - 2\n",
    "learning_rate = 0.05\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Create a list that contains errors of all data points for each epoch\n",
    "    epoch_errors = []\n",
    "    \n",
    "    for x, y in zip(X, Y):     # x의 shape:(4,),  y의 shape:(3,)        \n",
    "        \n",
    "        W_n = [np.random.randn(l, next_l) for l, next_l in zip(structure[:-1], structure[1:])]        \n",
    "        B_n = [np.random.randn(l, 1) for l in structure[1:]]\n",
    "        \n",
    "        # Forward Propagate\n",
    "        # Create two lists that contain pre and post activation vector of each layer, respectively\n",
    "        Z_n, A_n = [], []\n",
    "        \n",
    "        for i, (b, W) in enumerate(zip(B_n, W_n)):\n",
    "            if i == 0:\n",
    "                z = np.dot(np.array(W).T, x).reshape(-1, 1) + np.array(b)\n",
    "            else:\n",
    "                z = np.dot(np.array(W).T, a).reshape(-1, 1) + np.array(b)\n",
    "            a = sigmoid(z)\n",
    "\n",
    "            Z_n.append(z)\n",
    "            A_n.append(a)\n",
    "        # print(Z_n) \n",
    "        # print()  \n",
    "    \n",
    "    \n",
    "        # Backpropagate\n",
    "        # Initialize a list called e_Je_W_ns that will contain e_Je_W matrices for each layer\n",
    "        e_Je_W_ns = [np.zeros(W.shape) for W in W_n]    # (4, 2), (2, 2), (2, 3)\n",
    "        #for x in e_Je_W_ns:\n",
    "        #    print(x.shape)\n",
    "        #print()\n",
    "\n",
    "        # Initialize a list called e_Je_B_ns that will contain e_Je_B vectors for each layer\n",
    "        e_Je_B_ns = [np.zeros(B.shape) for B in B_n]    # (2, 1), (2, 1), (3, 1)\n",
    "        \n",
    "        for L in range(H, -1, -1):\n",
    "            if L != H:\n",
    "                delta = sigmoid_derivative(Z_n[L]) * np.dot(W_n[L+1], delta)\n",
    "            else:\n",
    "                delta = sigmoid_derivative(Z_n[L]) * (A_n[L] - y.reshape(-1, 1))\n",
    "                \n",
    "            e_Je_B_ns[L] = delta\n",
    "            # print(f\"{L} : {delta}\")\n",
    "            \n",
    "            if L != 0:\n",
    "                e_Je_W_ns[L] = np.dot(A_n[L-1], delta.T)\n",
    "            else:\n",
    "                e_Je_W_ns[L] = np.dot(x.reshape(-1, 1), delta.T)\n",
    "    \n",
    "        \"\"\"\n",
    "        for x in e_Je_W_ns:\n",
    "            print(x)\n",
    "        print()\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        for i, (wn, ejew, bn, ejeb) in enumerate(zip(W_n, e_Je_W_ns, B_n, e_Je_B_ns)):\n",
    "            W_n[i] -= learning_rate/len(X) * ejew\n",
    "            B_n[i] -= learning_rate/len(X) * ejeb\n",
    "            \"\"\"\n",
    "            print(wn)\n",
    "            print(ejew)\n",
    "            print(bn)\n",
    "            print(ejeb)\n",
    "            print()\n",
    "            \"\"\"    \n",
    "    \n",
    "        datapoint_error = calculate_loss(A_n[-1], y)\n",
    "        epoch_errors.append(datapoint_error)\n",
    "    total_error_epoch = np.sum(epoch_errors)\n",
    "    print(f\"{epoch}th total error : {total_error_epoch:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b18c970f1c267b2ae7de1f51c7b9ea29dc67f6c9f87d5b0ef47c4534e0830b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
